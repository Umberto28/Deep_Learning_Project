{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "from torchvision.transforms.functional import pil_to_tensor, resize\n",
    "from torch.nn.functional import pad\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from sklearn import preprocessing\n",
    "from path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IAM_statistics():\n",
    "    h = []\n",
    "    w = []\n",
    "    # images = []\n",
    "    set_dir = f'IAM/sentences'\n",
    "    for dir in os.listdir(set_dir):\n",
    "        dir_path = os.path.join(set_dir, dir)\n",
    "        for subdir in os.listdir(dir_path):\n",
    "            subdir_path = os.path.join(dir_path, subdir)\n",
    "            for image in os.listdir(subdir_path):\n",
    "                img_path = os.path.join(subdir_path, image)\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                # images.append(resize(pil_to_tensor(img) / 255., (128, 1024)))\n",
    "                img_size = img.size\n",
    "                h.append(img_size[1])\n",
    "                w.append(img_size[0])\n",
    "    # images = torch.stack(images, dim=0)\n",
    "    # mean = torch.mean(images)\n",
    "    # std = torch.std(images)\n",
    "\n",
    "    # print(mean, std)\n",
    "    \n",
    "    return max(w), max(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_statistics(base):\n",
    "    h = []\n",
    "    w = []\n",
    "    lines = DYSG / f'{base}/original'\n",
    "    for author in os.listdir(lines):\n",
    "        aut_dir = os.path.join(lines, author)\n",
    "        for line in os.listdir(aut_dir):\n",
    "            img_path = os.path.join(aut_dir, line)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            # images.append(resize(pil_to_tensor(img) / 255., (128, 1024)))\n",
    "            img_size = img.size\n",
    "            h.append(img_size[1])\n",
    "            w.append(img_size[0])\n",
    "    \n",
    "    return max(w), max(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_splits(path):\n",
    "    if os.path.isdir(os.path.join(path, 'train.txt')):\n",
    "        return\n",
    "    else:\n",
    "        print(\"Creating Simple splits.\")\n",
    "    dis = [filename for filename in os.listdir(path) if 'X' in filename]\n",
    "    not_dis = [filename for filename in os.listdir(path) if 'O' in filename]\n",
    "\n",
    "    test_dis = random.sample(dis, 3)\n",
    "    validation = [random.choice(test_dis)]\n",
    "    test_dis.remove(validation[0])\n",
    "\n",
    "    test_not_dis = random.sample(not_dis, 3)\n",
    "    validation.append(random.choice(test_not_dis))\n",
    "    test_not_dis.remove(validation[1])\n",
    "\n",
    "    test_dis.extend(test_not_dis)\n",
    "    train = [filename for filename in os.listdir(path) if filename not in test_dis]\n",
    "\n",
    "    with open(os.path.join('/'.join(path.split(\"/\")[:-1]), 'train.txt'), 'w') as f:\n",
    "        for t in train:\n",
    "            f.write(f\"{t}\\n\")\n",
    "    \n",
    "    with open(os.path.join('/'.join(path.split(\"/\")[:-1]), 'validation.txt'), 'w') as f:\n",
    "        for t in validation:\n",
    "            f.write(f\"{t}\\n\")\n",
    "\n",
    "    with open(os.path.join('/'.join(path.split(\"/\")[:-1]), 'test.txt'), 'w') as f:\n",
    "        for t in test_dis:\n",
    "            f.write(f\"{t}\\n\")\n",
    "\n",
    "def create_multiple_splits(path, labels):\n",
    "    if not os.path.isdir(os.path.join(path, 'splits')):\n",
    "        os.mkdir(os.path.join(path, 'splits'))\n",
    "    columns = ['CERTIFIED', 'EXPERT', 'PROFESSORS']\n",
    "    labels = pd.read_csv(labels, header=0, index_col=0, sep=\";\")\n",
    "\n",
    "    for column in columns:\n",
    "        if os.path.isdir(os.path.join(path, 'splits', column)):\n",
    "            continue\n",
    "        else:\n",
    "            print(f'Creating {column} split.')\n",
    "            os.mkdir(os.path.join(path, 'splits', column))\n",
    "        column_dis = [name for name in labels.loc[labels[column] >= 0.5].index.tolist()]\n",
    "        column_not_dis = [name for name in labels.loc[labels[column] < 0.5].index.tolist()]\n",
    "        if column == 'CERTIFIED': lenght_splits = [int(len(column_dis) / 4), int(len(column_dis) / 4), int(len(column_dis) / 4), len(column_dis) - int(len(column_dis) / 4)*3]\n",
    "        elif column == 'EXPERT': lenght_splits = [4, 4, 4, 3]\n",
    "        elif column == 'PROFESSORS': lenght_splits = [5, 5, 5, 4]\n",
    "        else: break\n",
    "        random.shuffle(column_dis)\n",
    "        it_column_dis = iter(column_dis)\n",
    "        tests = [list(islice(it_column_dis, elem)) for elem in lenght_splits]\n",
    "        for t, l in enumerate(lenght_splits):\n",
    "            # selection = column_not_dis.pop(column_not_dis.index(random.sample(column_not_dis, l)))\n",
    "            selection = random.sample(column_not_dis, l)\n",
    "            [column_not_dis.remove(s) for s in selection]\n",
    "            tests[t].extend(selection)\n",
    "        \n",
    "        for t, test in enumerate(tests):\n",
    "            split = os.path.join(path, 'splits', column, f'split{t}')\n",
    "            os.mkdir(split)\n",
    "            training = []\n",
    "            [training.extend(tt) for i, tt in enumerate(tests) if i != t]\n",
    "            column_not_dis_copy = column_not_dis.copy()\n",
    "            validation = [random.sample(training, 1)[0], random.sample(column_not_dis_copy, 1)[0]]\n",
    "            training.remove(validation[0]), column_not_dis_copy.remove(validation[1])\n",
    "            training.extend(column_not_dis_copy)\n",
    "            print(\"Union:\",len(training), len(validation), len(test), len(training) + len(validation) + len(test))\n",
    "\n",
    "            with open(os.path.join(split, \"train.txt\"), 'w') as output:\n",
    "                for row in training:\n",
    "                    output.write(str(row) + '\\n')\n",
    "            \n",
    "            with open(os.path.join(split, \"validation.txt\"), 'w') as output:\n",
    "                for row in validation:\n",
    "                    output.write(str(row) + '\\n')\n",
    "            \n",
    "            with open(os.path.join(split, \"test.txt\"), 'w') as output:\n",
    "                for row in test:\n",
    "                    output.write(str(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_authors_per_set():\n",
    "    DATA = 'IAM/DATA'\n",
    "    XML = 'IAM/xml'\n",
    "    SETS_PATH = 'IAM/SETS'\n",
    "\n",
    "    for set in SETS:\n",
    "        set_dir = f'IAM/{set.split(\".\")[0]}'\n",
    "        if not os.path.isdir(set_dir):\n",
    "            os.mkdir(f'IAM/{set.split(\".\")[0]}')\n",
    "        set = os.path.join(SETS_PATH, set)\n",
    "        set_samples = [line.rstrip('\\n') for line in open(set, 'r')]\n",
    "        for f in set_samples:\n",
    "            subdir = '-'.join(f.split(\"-\")[:2])\n",
    "            tree = ET.parse(os.path.join(XML, subdir + '.xml'))\n",
    "            root = tree.getroot()\n",
    "            writer = root.attrib['writer-id']\n",
    "            print('author:', writer, end='\\r')\n",
    "            if not os.path.isdir(os.path.join(set_dir, writer)):\n",
    "                os.mkdir(os.path.join(set_dir, writer))\n",
    "\n",
    "            dir = f.split(\"-\")[0]\n",
    "\n",
    "            for png in os.listdir(os.path.join(DATA, dir, subdir)):\n",
    "                shutil.copy(os.path.join(DATA, dir, subdir, png), \n",
    "                            os.path.join(set_dir, writer, png))\n",
    "\n",
    "def get_bhk_features(filename = os.path.join(DYSG,'children/original/A01_1cb57/row3_O_1cb57.png'), base = 'children', bhk = 'binary'):\n",
    "    # read\n",
    "    assert bhk == 'binary' or bhk == 'float' or bhk == 'double'\n",
    "    author = filename.split(\"/\")[-2]\n",
    "    line = filename.split(\"/\")[-1].split(\"_\")[0]\n",
    "    csv_path = CSVS / f'{base}_{bhk}.csv'\n",
    "    df = pd.read_csv(csv_path, header=0, index_col=0)\n",
    "    # print(csv_path)\n",
    "    # normalize\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(df.values)\n",
    "    norm_df = pd.DataFrame(x_scaled)\n",
    "    norm_df.columns = df.columns\n",
    "    norm_df.index = df.index\n",
    "\n",
    "    # get features\n",
    "    global_features = torch.tensor(norm_df.filter(like='global').loc[author].to_numpy(), dtype=torch.float32)\n",
    "    line_features = torch.tensor(norm_df.filter(like=line).loc[author].to_numpy(), dtype=torch.float32)\n",
    "    if line_features.shape[0] == 29: line_features = pad(line_features, (0, 7))\n",
    "    features = torch.cat((global_features, line_features))\n",
    "    return features, features.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
