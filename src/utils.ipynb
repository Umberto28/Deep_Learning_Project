{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Funzioni utili**\n",
    "Definiamo alcune funzioni per la suddivisione ed il calcolo di alcune statistiche dei dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "from math import floor\n",
    "%run path.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Statistiche IAM**\n",
    "Questa funzione scorre tutto il dataset IAM e calcola le dimensioni massime raggiunte dalle immagini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IAM_statistics():\n",
    "    h = []\n",
    "    w = []\n",
    "    # images = []\n",
    "    main_dir = IAM + '/sentences'\n",
    "    for dir in os.listdir(main_dir):\n",
    "        dir_path = os.path.join(main_dir, dir)\n",
    "        for subdir in os.listdir(dir_path):\n",
    "            subdir_path = os.path.join(dir_path, subdir)\n",
    "            for image in os.listdir(subdir_path):\n",
    "                img_path = os.path.join(subdir_path, image)\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                # images.append(resize(pil_to_tensor(img) / 255., (128, 1024)))\n",
    "                img_size = img.size\n",
    "                h.append(img_size[1])\n",
    "                w.append(img_size[0])\n",
    "    # images = torch.stack(images, dim=0)\n",
    "    # mean = torch.mean(images)\n",
    "    # std = torch.std(images)\n",
    "\n",
    "    # print(mean, std)\n",
    "    \n",
    "    return max(w), max(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Statistiche Dysgraphia**\n",
    "Questa funzione scorre tutto il dataset Dysgraphia e calcola le dimensioni massime raggiunte dalle immagini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_statistics(aug : str = 'not_aug'):\n",
    "    h = []\n",
    "    w = []\n",
    "    \n",
    "    if aug == 'aug':\n",
    "        main_dir = ADYSG\n",
    "    else:\n",
    "        main_dir = DYSG\n",
    "    \n",
    "    for dir in os.listdir(main_dir):\n",
    "        dir_path = os.path.join(main_dir, dir)\n",
    "        for image in os.listdir(dir_path):\n",
    "            img_path = os.path.join(dir_path, image)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            # images.append(resize(pil_to_tensor(img) / 255., (128, 1024)))\n",
    "            img_size = img.size\n",
    "            h.append(img_size[1])\n",
    "            w.append(img_size[0])\n",
    "    # images = torch.stack(images, dim=0)\n",
    "    # mean = torch.mean(images)\n",
    "    # std = torch.std(images)\n",
    "\n",
    "    # print(mean, std)\n",
    "    \n",
    "    return max(w), max(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Suddivisione Dysgraphia**\n",
    "Questa funzione suddivide il dataset etichettato Dysgraphia in train, test e validation set e li salva in file di testo, successivamente considerati dall'architettura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Simple splits.\n"
     ]
    }
   ],
   "source": [
    "def create_simple_splits(data_path : str, aug : str, train_split = 0.7, val_split = 0.15):\n",
    "    if os.path.isdir(os.path.join(SPLIT, f'train_{aug}.txt')):\n",
    "        return\n",
    "    else:\n",
    "        print(\"Creating Simple splits.\")\n",
    "    \n",
    "    dys = []\n",
    "    not_dys = []\n",
    "    \n",
    "    for path in os.listdir(data_path):\n",
    "        if \"No_Dysgraphic\" in path:\n",
    "            not_dys = not_dys + [data_path+\"/\"+path+\"/\"+filename for filename in os.listdir(data_path + \"/\" + path)]\n",
    "        else:\n",
    "            dys = dys + [data_path+\"/\"+path+\"/\"+filename for filename in os.listdir(data_path + \"/\" + path)]\n",
    "\n",
    "    random.shuffle(dys)\n",
    "    random.shuffle(not_dys)\n",
    "\n",
    "    # Calcolliamo le dimensioni dei set\n",
    "    dys_train_size = floor(train_split * len(dys))\n",
    "    dys_val_size = floor(val_split * len(dys))\n",
    "    \n",
    "    non_dys_train_size = floor(train_split * len(not_dys))\n",
    "    non_dys_val_size = floor(val_split * len(not_dys))\n",
    "    \n",
    "    # Suddividiamo il dataset\n",
    "    dys_train = dys[:dys_train_size]\n",
    "    dys_val = dys[dys_train_size:dys_train_size + dys_val_size]\n",
    "    dys_test = dys[dys_train_size + dys_val_size:]\n",
    "    \n",
    "    not_dys_train = not_dys[:non_dys_train_size]\n",
    "    not_dys_val = not_dys[non_dys_train_size:non_dys_train_size + non_dys_val_size]\n",
    "    not_dys_test = not_dys[non_dys_train_size + non_dys_val_size:]\n",
    "\n",
    "    # Uniamo i set dys e not_dys\n",
    "    train_set = dys_train + not_dys_train\n",
    "    val_set = dys_val + not_dys_val\n",
    "    test_set = dys_test + not_dys_test\n",
    "    \n",
    "    # Mescoliamo ulteriormente i set finali\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(val_set)\n",
    "    random.shuffle(test_set)\n",
    "\n",
    "    with open(os.path.join(SPLIT, f'train_{aug}.txt'), 'w') as f:\n",
    "        for t in train_set:\n",
    "            f.write(f\"{t}\\n\")\n",
    "    \n",
    "    with open(os.path.join(SPLIT, f'val_{aug}.txt'), 'w') as f:\n",
    "        for t in val_set:\n",
    "            f.write(f\"{t}\\n\")\n",
    "\n",
    "    with open(os.path.join(SPLIT, f'test_{aug}.txt'), 'w') as f:\n",
    "        for t in test_set:\n",
    "            f.write(f\"{t}\\n\")\n",
    "\n",
    "# dataset_path = DYSG\n",
    "# create_simple_splits(dataset_path, 'no_aug')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
