{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pre-training**\n",
    "Definiamo la funzione di pre-train, in cui il modello considerato viene addestrato sul dataset IAM e salvato nella cartella checkpoint. Da l√¨ poi viene ripreso per il training sul nostro dataset etichettato Dysgraphia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import TripletMarginLoss\n",
    "import torch.optim as optim\n",
    "import time\n",
    "%run path.ipynb\n",
    "from datetime import timedelta\n",
    "%run model.ipynb\n",
    "%run data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "\n",
    "    # LOAD DATA\n",
    "    train_data = IAMDL('train', IAM)\n",
    "    validation_data = IAMDL('validation', IAM)\n",
    "\n",
    "    # LOAD MODEL\n",
    "    # wrapper = ViTWrapper('pretrain', DEVICE, pretrain=True)\n",
    "    wrapper = ResnetWrapper(name='resnet18', device=DEVICE, classes = 2, pretrain=True)\n",
    "    model = wrapper.get_model()\n",
    "\n",
    "    # TRAIN SETTINGS\n",
    "    epochs = EPOCHS\n",
    "    start_epoch = START_EPOCH\n",
    "    batch_size = BATCH_SIZE\n",
    "    best_val_loss = BEST_VAL_LOSS\n",
    "    exit_counter = EXIT_COUNTER\n",
    "    lr = LR\n",
    "\n",
    "    loss = TripletMarginLoss(margin=1.0, p=2)\n",
    "    opt = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    if args.resume:\n",
    "        start_epoch, best_val_loss, exit_counter, opt_chk = wrapper.resume('pretrain_checkpoint.pth')\n",
    "        opt.load_state_dict(opt_chk)\n",
    "\n",
    "    print('Start Training')\n",
    "    for e in range(start_epoch, epochs):\n",
    "\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        loader = iter(DataLoader(train_data, batch_size=batch_size, shuffle=True))\n",
    "        train_loss = 0.0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        start = time.time()\n",
    "        for i in range(0, int(len(train_data) / batch_size) + 1):\n",
    "            samples = next(loader)\n",
    "            a, p, n = train_data.batch_triplets(samples)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            anchor_embed, positive_embed, negative_embed  = model(a), model(p), model(n)\n",
    "\n",
    "            out = loss(anchor_embed, positive_embed, negative_embed)\n",
    "            out.backward()\n",
    "            opt.step()\n",
    "\n",
    "            train_loss += out.item()\n",
    "            running_loss += out.item()\n",
    "            if i % 5 == 4:\n",
    "                print(f'Epoch {e + 1} - Batch {i + 1} - Running Loss {running_loss / 5}' , end='\\r')\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        train_loss = train_loss / i\n",
    "        print(f'Epoch {e + 1} - Train Loss {train_loss} - Time {str(timedelta(seconds=time.time() - start))}')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            loader = iter(DataLoader(validation_data, batch_size=batch_size, shuffle=False))\n",
    "            for i in range(0, int(len(validation_data) / batch_size) + 1):\n",
    "                samples = next(loader)\n",
    "                a, p, n = validation_data.batch_triplets(samples)\n",
    "                anchor_embed, positive_embed, negative_embed = model(a), model(p), model(n)\n",
    "\n",
    "                out = loss(anchor_embed, positive_embed, negative_embed)\n",
    "                val_loss += out.item()\n",
    "            \n",
    "            val_loss = val_loss / i\n",
    "            print(f\"Validation Loss {val_loss}\")\n",
    "            if val_loss < best_val_loss:\n",
    "                print(f\"    !- Validation improovement! {best_val_loss} -> {val_loss}\")\n",
    "                exit_counter = 0\n",
    "                best_val_loss = val_loss\n",
    "                is_best = True\n",
    "            else:\n",
    "                print(f\"    !- No improovement!\")\n",
    "                is_best = False\n",
    "                exit_counter += 1\n",
    "            \n",
    "            state = {\n",
    "                'epoch': e + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'exit_counter': exit_counter,\n",
    "                'optimizer': opt.state_dict()\n",
    "            }\n",
    "            wrapper.save_state(state, is_best)\n",
    "        \n",
    "        if exit_counter == 20:\n",
    "            print(\"Exit\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Training')\n",
    "    parser.add_argument('--resume', '-r', action=\"store_true\",\n",
    "                        help=\"resume training\")\n",
    "    train(parser.parse_args([]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
